#!/usr/bin/env python

import os
import sys
import uuid
import tempfile
import errno
import yaml
import json
import shutil
import argparse
import collections
from string import Template
from collections import defaultdict
from subprocess import check_call
from copy import deepcopy
from urlparse import urlparse

THIS_DIR = os.path.abspath(os.path.dirname(__file__))
ROOT_DIR = os.path.dirname(THIS_DIR)
BASEPLAN_DIR = os.path.join(ROOT_DIR, "src", "baseplan")

PYONAPP_PREFIX = "pyonapp"
HAAGENT_DEFINITION_NAME = "haagent"


class PlanError(Exception):
    """Generic error raised by plan generator
    """


class PathFinder(object):
    """Utility object used to find launch plan component files
    """

    def get_baseplan_path(self):
        """Returns path to baseplan directory
        """
        return BASEPLAN_DIR

    def get_template_path(self, template):
        """Returns path to specified template file

        Raises PlanError if template doesn't exist
        """
        path = os.path.join(ROOT_DIR, "src", "templates", template)

        if not os.path.exists(path):
            raise PlanError("template file does not exist: %s" % path)
        return path

    def get_template_string(self, template):
        path = self.get_template_path(template)
        try:
            with open(path) as f:
                return f.read()

        except Exception, e:
            raise PlanError(
                "Problem opening '%s'. Cannot proceed. Error: %s" %
                (path, str(e)))

    def get_template(self, template):
        return Template(self.get_template_string(template))

    def get_dt_path(self):
        """Returns path to deployable types directory
        """
        return os.path.join(ROOT_DIR, "src", "dt")

    def get_profile_defaults(self):
        path = os.path.join(ROOT_DIR, "src", "profile_defaults.yml")
        try:
            with file(path) as f:
                return yaml.load(f)
        except Exception, e:
            raise PlanError("Problem reading profile config defaults file '%s': %s" %
                (path, str(e)))

    def get_pyon_config(self):
        path = os.path.join(ROOT_DIR, "src", "pyon.yml")
        try:
            with file(path) as f:
                return yaml.load(f)
        except Exception, e:
            raise PlanError("Problem reading pyon config defaults '%s': %s" %
                (path, str(e)))


##############################################################################
# LAUNCH PROFILES
##############################################################################

def _get_or_bail(config, key, prefix="", empty_ok=False):
    if prefix:
        prefix = "%s." % prefix
    if not key in config:
        raise PlanError("Profile config missing '%s%s'" % (prefix, key))
    value = config[key]

    if not empty_ok and not value:
        raise PlanError("Profile config value '%s%s' is empty" % (prefix, key))

    return value


def _make_csv_list(value):
    if isinstance(value, basestring):
        return value

    if value is None:
        return ""

    if isinstance(value, (list, tuple)):
        return ",".join(value)

    raise PlanError("Expected sequence value, got: %s" % value)


def _write_plan_configs(template, output_dir, bootlevel_config_paths,
    global_vars=None, extra_vars=None, initial_filename="launch.conf",
    restart_filename="restart.conf"):

    last_level = get_last_level(template.template)

    # make a copy and append the finalize level to the end
    bootlevel_config_paths = list(bootlevel_config_paths)
    bootlevel_config_paths.append("finalize/finalize.conf")

    new_levels_lines = [""]  # blank line to start with
    for bootlevel_config in bootlevel_config_paths:
        new_levels_lines.append("level%s: %s" % (last_level,
            bootlevel_config))
        last_level += 1

    new_levels = "\n".join(new_levels_lines)

    global_vars = deepcopy(global_vars) if global_vars else {}

    def _get_global_vars_str(vars):
        return "\n".join("%s: %s" % (key, vars[key])
            for key in sorted(vars.keys()))

    global_vars['bootmode'] = "initial"
    initial_global_vars_str = _get_global_vars_str(global_vars)
    global_vars['bootmode'] = "restart"
    restart_global_vars_str = _get_global_vars_str(global_vars)

    tmpl_vars = {"levels": new_levels}
    if extra_vars:
        tmpl_vars.update(extra_vars)

    # write out a launch.conf and restart.conf
    tmpl_vars['globals'] = initial_global_vars_str
    with open(os.path.join(output_dir, initial_filename), 'w') as f:
        f.write(template.substitute(tmpl_vars))

    tmpl_vars['globals'] = restart_global_vars_str
    with open(os.path.join(output_dir, restart_filename), 'w') as f:
        f.write(template.substitute(tmpl_vars))


def _set_deployment_pd_config(deployment, pd_config):
    pd_config['engines'] = deepcopy(deployment.execution_engines)
    pd_config['default_engine'] = deployment.default_execution_engine_id


def _get_rabbitmq_global_vars(config, need_host=False):
    rabbitmq = _get_or_bail(config, "rabbitmq")

    global_vars = {}
    global_vars['rabbitmq_username'] = _get_or_bail(rabbitmq, "username", "rabbitmq")
    global_vars['rabbitmq_password'] = _get_or_bail(rabbitmq, "password", "rabbitmq")
    if need_host:
        global_vars['rabbitmq_host'] = _get_or_bail(rabbitmq, "host", "rabbitmq")

    return global_vars


def _get_couchdb_global_vars(config, need_host=False):
    couchdb = _get_or_bail(config, "couchdb")

    global_vars = {}
    global_vars['couchdb_username'] = _get_or_bail(couchdb, "username", "couchdb", empty_ok=True)
    global_vars['couchdb_password'] = _get_or_bail(couchdb, "password", "couchdb", empty_ok=True)
    if need_host:
        global_vars['couchdb_host'] = _get_or_bail(couchdb, "host", "couchdb")

    return global_vars


def _get_zookeeper_global_vars(config):
    global_vars = {}

    zookeeper = config.get('zookeeper')
    if zookeeper:
        zookeeper_hosts = _get_or_bail(zookeeper, 'hosts', 'zookeeper')
        global_vars['zookeeper_hosts'] = _make_csv_list(zookeeper_hosts)
        global_vars['zookeeper_enabled'] = str(bool(zookeeper.get('enabled', True)))
    else:
        global_vars['zookeeper_hosts'] = ""
        global_vars['zookeeper_enabled'] = str(False)
    return global_vars


def _get_common_global_vars(config):
    global_vars = {}
    pyon_config_output = config.get('pyon_config_output')
    if pyon_config_output:
        pyon_config_output = os.path.expanduser(pyon_config_output)
        global_vars['pyon_config_output'] = pyon_config_output
    else:
        global_vars['pyon_config_output'] = ""

    trafficsentinel = config.get('trafficsentinel')
    if trafficsentinel:
        trafficsentinel_host = trafficsentinel.get('host')
        if trafficsentinel_host:
            global_vars['hsflow_collector_host'] = trafficsentinel_host

    global_vars.update(_get_zookeeper_global_vars(config))

    return global_vars


def _get_common_vm_global_vars(config):
    global_vars = {}
    packages = _get_or_bail(config, 'packages')
    for package, url in packages.items():
        global_vars["%s_archive_url" % package] = url

    global_vars['gateway_ddns_hosts'] = _make_csv_list(
        config.get('gateway_ddns_hosts', ""))

    graylog_host = ""
    graylog_config = config.get('graylog')
    if graylog_config:
        graylog_host = graylog_config.get('host', "")
    global_vars['graylog_host'] = graylog_host

    return global_vars


class LocalProfile(object):
    """Lightweight launch on local system (no VMs)
    """
    PLAN_CONFIG_TEMPLATE = "local.conf"

    # prototype eeagent definition copied and filled in for each agent
    _EEAGENT_PROTOTYPE = {
        "config": {
            "system": {
              "name": "${sysname}"
            },
            "eeagent": {
                "heartbeat": 10,
                "heartbeat_queue": "processdispatcher_heartbeats",
                "launch_type": {
                    "name": "pyon",
                    "pyon_directory": "${pyon_path}",
                    "persistence_directory": "${persistence_dir}"
                }
            }
        }
    }

    def __init__(self, pathfinder):
        self.pathfinder = pathfinder

    def load_config(self, config):
        global_vars = _get_common_global_vars(config)
        global_vars.update(_get_rabbitmq_global_vars(config, need_host=True))
        global_vars.update(_get_couchdb_global_vars(config, need_host=True))
        pyon_path = _get_or_bail(config, 'pyon_path')
        global_vars['pyon_path'] = os.path.expanduser(pyon_path)

        self.global_vars = global_vars

    def write_plan(self, deployment, output_dir, bootlevel_config_paths):
        self._write_engine_config(deployment, output_dir)

        template = self.pathfinder.get_template(self.PLAN_CONFIG_TEMPLATE)
        _write_plan_configs(template, output_dir, bootlevel_config_paths,
            global_vars=self.global_vars)

    def _write_engine_config(self, deployment, output_dir):
        tmpl_name = "local-deployment.json"
        tmpl = self.pathfinder.get_template_string(tmpl_name)
        deployment_dict = json.loads(tmpl)

        # need to write some deployment information in two places for
        # lightweight launch. First, the usual PD engine config:

        process_dispatchers = deployment_dict['pyon-process-dispatchers']
        if len(process_dispatchers) != 1:
            raise PlanError("only a single process dispatcher is supported. %s has %d"
                % (tmpl_name, len(process_dispatchers)))
        process_dispatcher_name = process_dispatchers.keys()[0]
        if process_dispatcher_name != "process_dispatcher":
            # we can relax this once we fixup the dashi queue naming for PD
            raise PlanError("process dispatcher name must be 'process_dispatcher'")
        process_dispatcher = process_dispatchers[process_dispatcher_name]
        pd_cfg = process_dispatcher['config']['processdispatcher']

        _set_deployment_pd_config(deployment, pd_cfg)

        # additionally we create fake nodes and eeagents in the config

        nodes = deployment_dict['pyon-nodes'] = {}
        node_index = 0
        for engine_id, engine in deployment.execution_engines.iteritems():
            for _ in range(engine['base_need']):
                nodename = "node%s" % node_index
                node_index += 1

                eeagents = {}
                node = {
                    'engine': engine_id,
                    'process-dispatcher': process_dispatcher_name,
                    'eeagents': eeagents
                }
                nodes[nodename] = node

                for replica_index in range(engine.get('replicas', 1)):
                    eeagent_name = "%s_eeagent%s" % (nodename, replica_index)
                    # start with a prototype eeagent config and fill in
                    eeagent = deepcopy(self._EEAGENT_PROTOTYPE)

                    eeagent_cfg = eeagent['config']
                    eeagent_cfg['agent'] = {'resource_id': eeagent_name}
                    eeagent_cfg['eeagent']['name'] = eeagent_name
                    eeagent_cfg['eeagent']['slots'] = engine['slots']

                    eeagents[eeagent_name] = eeagent

        basenode_dir = os.path.join(output_dir, "basenode-local")
        if not os.path.exists(basenode_dir):
            raise PlanError("Expected local basenode directory does not exist: %s" % basenode_dir)
        deployment_path = os.path.join(basenode_dir, "pyon-deployment.json")

        with open(deployment_path, 'w') as f:
            json.dump(deployment_dict, f, sort_keys=True, indent=2)


class BaseVMProfile(object):

    def __init__(self, pathfinder):
        self.pathfinder = pathfinder

    def _write_engine_config(self, deployment, output_dir):
        pd_dict = json.loads(self.pathfinder.get_template_string("pd.json"))

        pd_cfg = pd_dict['pyon']['run_config']['config']['processdispatcher']

        _set_deployment_pd_config(deployment, pd_cfg)

        pd_dir = os.path.join(output_dir, "pd")
        if not os.path.exists(pd_dir):
            raise PlanError("Expected Process Dispatcher bootlevel directory does not exist: %s" % pd_dir)
        pd_json_path = os.path.join(pd_dir, "pd.json")

        with open(pd_json_path, 'w') as f:
            json.dump(pd_dict, f, sort_keys=True, indent=2)

    def _write_dtrs_bootstrap(self, output_dir, site, creds, base_image, allocation):

        dt_bootstrap_dir = os.path.join(output_dir, "dtrs-bootstrap", "dt-bootstrap")

        site_name = str(site['name'])
        site_path = os.path.join(dt_bootstrap_dir, 'sites', site_name + ".yml")
        with open(site_path, 'w') as f:
            yaml.dump(site, f, default_flow_style=False)

        creds_path = os.path.join(dt_bootstrap_dir, 'credentials', site_name + ".yml")
        with open(creds_path, 'w') as f:
            yaml.dump(creds, f, default_flow_style=False)

        dt_path = self.pathfinder.get_dt_path()
        for dt_file in os.listdir(dt_path):
            if os.path.splitext(dt_file)[1].lower() == ".yml":
                with open(os.path.join(dt_path, dt_file)) as f:
                    dt = yaml.load(f)

                mappings = {site_name: {'iaas_image': base_image, 'iaas_allocation': allocation}}
                dt['mappings'] = mappings

                with open(os.path.join(dt_bootstrap_dir, "dt", dt_file), 'w') as f:
                    yaml.dump(dt, f, default_flow_style=False)

        make_tarball_exe = os.path.join(output_dir, "dtrs-bootstrap",
            "prepare-tarball.sh")
        check_call(make_tarball_exe)


class BaseNimbusProfile(BaseVMProfile):

    def load_config(self, config):

        # validate and parse profile config. pull out top level plan variables
        # and site/cred objects
        iaas_config = _get_or_bail(config, 'iaas')
        site_name = _get_or_bail(iaas_config, 'site', 'iaas')
        url = _get_or_bail(iaas_config, 'url', 'iaas')
        key = _get_or_bail(iaas_config, 'key', 'iaas')
        secret = _get_or_bail(iaas_config, 'secret', 'iaas')
        sshkey = _get_or_bail(iaas_config, 'sshkeyname', 'iaas')
        baseimage = _get_or_bail(iaas_config, 'base-image', 'iaas')
        baseallocation = _get_or_bail(iaas_config, 'base-allocation', 'iaas')

        try:
            url_parts = urlparse(url)
        except Exception, e:
            raise PlanError("Failed to parse IaaS url '%s': %s", url, e)

        host = url_parts.hostname
        port = url_parts.port

        # build site object
        self.site = {'name': site_name, 'description': '',
            'driver_class': 'libcloud.compute.drivers.ec2.NimbusNodeDriver',
            'driver_kwargs': {'host': host, 'port': port}}

        # build creds object
        self.creds = {'access_key': key, 'secret_key': secret, 'key_name': sshkey}

        # build plan template vars
        self.plan_vars = {'iaas_key': key, 'iaas_secret': secret,
            'iaas_url': url, 'image': baseimage, 'allocation': baseallocation,
            'sshkeyname': sshkey}

        self.baseimage = baseimage
        self.baseallocation = baseallocation

        global_vars = self._get_base_global_vars(config)
        global_vars['ctxbroker_host'] = host
        global_vars['ctxbroker_key'] = key
        global_vars['ctxbroker_secret'] = secret
        global_vars['iaas_site'] = site_name

        self.global_vars = global_vars

    def write_plan(self, deployment, output_dir, bootlevel_config_paths):
        self._write_engine_config(deployment, output_dir)
        self._write_dtrs_bootstrap(output_dir, self.site, self.creds,
            self.baseimage, self.baseallocation)
        template = self.pathfinder.get_template(self.PLAN_CONFIG_TEMPLATE)
        _write_plan_configs(template, output_dir, bootlevel_config_paths,
            extra_vars=self.plan_vars, global_vars=self.global_vars)


class NimbusDynamicProfile(BaseNimbusProfile):
    """Complete launch on Nimbus cloud
    """
    PLAN_CONFIG_TEMPLATE = "nimbus-dynamic.conf"

    def _get_base_global_vars(self, config):
        global_vars = _get_common_global_vars(config)
        global_vars.update(_get_common_vm_global_vars(config))
        global_vars.update(_get_rabbitmq_global_vars(config, need_host=False))
        global_vars.update(_get_couchdb_global_vars(config, need_host=False))
        return global_vars


class NimbusStaticProfile(BaseNimbusProfile):
    """Launch on Nimbus cloud with external dependencies
    """
    PLAN_CONFIG_TEMPLATE = "nimbus-static.conf"

    def _get_base_global_vars(self, config):
        global_vars = _get_common_global_vars(config)
        global_vars.update(_get_common_vm_global_vars(config))
        global_vars.update(_get_rabbitmq_global_vars(config, need_host=True))
        global_vars.update(_get_couchdb_global_vars(config, need_host=True))
        return global_vars


class BaseEC2Profile(BaseVMProfile):

    #TODO add others. Or better yet, lift this mapping into the provisioner/dtrs?
    _LIBCLOUD_EC2_REGION_DRIVERS = {
        "us-west-1": "libcloud.compute.drivers.ec2.EC2NodeDriver",
        "us-east-1": "libcloud.compute.drivers.ec2.EC2USWestNodeDriver",
    }

    def load_config(self, config):

        # validate and parse profile config. pull out top level plan variables
        # and site/cred objects
        iaas_config = _get_or_bail(config, 'iaas')
        site_name = _get_or_bail(iaas_config, 'site', 'iaas')
        region = _get_or_bail(iaas_config, 'region', 'iaas')
        key = _get_or_bail(iaas_config, 'key', 'iaas')
        secret = _get_or_bail(iaas_config, 'secret', 'iaas')
        sshkey = _get_or_bail(iaas_config, 'sshkeyname', 'iaas')
        baseimage = _get_or_bail(iaas_config, 'base-image', 'iaas')
        baseallocation = _get_or_bail(iaas_config, 'base-allocation', 'iaas')

        driver_class = self._LIBCLOUD_EC2_REGION_DRIVERS.get(region)
        if not driver_class:
            raise PlanError("EC2 region unknown! It may need to be added to the (hardcoded) mapping.")

        # build site object
        self.site = {'name': site_name, 'description': 'EC2 %s region' % region,
            'driver_class': driver_class}

        # build creds object
        self.creds = {'access_key': key, 'secret_key': secret, 'key_name': sshkey}

        # build plan template vars
        self.plan_vars = {'iaas_key': key, 'iaas_secret': secret,
            'region': region, 'image': baseimage, 'allocation': baseallocation,
            'sshkeyname': sshkey}

        self.baseimage = baseimage
        self.baseallocation = baseallocation

        global_vars = self._get_base_global_vars(config)
        global_vars['ctxbroker_key'] = key
        global_vars['ctxbroker_secret'] = secret
        global_vars['iaas_site'] = site_name

        self.global_vars = global_vars

    def write_plan(self, deployment, output_dir, bootlevel_config_paths):
        self._write_engine_config(deployment, output_dir)
        self._write_dtrs_bootstrap(output_dir, self.site, self.creds,
            self.baseimage, self.baseallocation)
        template = self.pathfinder.get_template(self.PLAN_CONFIG_TEMPLATE)
        _write_plan_configs(template, output_dir, bootlevel_config_paths,
            extra_vars=self.plan_vars, global_vars=self.global_vars)


class EC2DynamicProfile(BaseEC2Profile):
    """Complete launch on EC2
    """
    PLAN_CONFIG_TEMPLATE = "ec2-dynamic.conf"

    def _get_base_global_vars(self, config):
        global_vars = _get_common_global_vars(config)
        global_vars.update(_get_common_vm_global_vars(config))
        global_vars.update(_get_rabbitmq_global_vars(config, need_host=False))
        global_vars.update(_get_couchdb_global_vars(config, need_host=False))
        return global_vars


class EC2StaticProfile(BaseEC2Profile):
    """Launch on EC2 with external dependencies
    """
    PLAN_CONFIG_TEMPLATE = "ec2-static.conf"

    def _get_base_global_vars(self, config):
        global_vars = _get_common_global_vars(config)
        global_vars.update(_get_common_vm_global_vars(config))
        global_vars.update(_get_rabbitmq_global_vars(config, need_host=True))
        global_vars.update(_get_couchdb_global_vars(config, need_host=True))
        return global_vars


LAUNCH_PROFILE_TYPES = {
    'local': LocalProfile,
    'nimbus-dynamic': NimbusDynamicProfile,
    'nimbus-static': NimbusStaticProfile,
    'ec2-dynamic': EC2DynamicProfile,
    'ec2-static': EC2StaticProfile
}


def get_last_level(cloudinitd_conf):
    """Get the number of the last level defined in a cloudinitd launch plan
    """

    i = 1
    while True:
        level = "level%s" % i
        if level not in cloudinitd_conf:
            if i == 1:
                msg = "There don't seem to be any levels in your launch plan. "
                msg += "There is probably something wrong with your config."
                raise PlanError(msg)
            else:
                return i
        else:
            i += 1


# dict_merge from: http://appdelegateinc.com/blog/2011/01/12/merge-deeply-nested-dicts-in-python/
def quacks_like_dict(object):
    """Check if object is dict-like"""
    return isinstance(object, collections.Mapping)


def dict_merge(a, b):
    assert quacks_like_dict(a)
    assert quacks_like_dict(b)

    dst = a.copy()

    stack = [(dst, b)]
    while stack:
        current_dst, current_src = stack.pop()
        for key in current_src:
            if key not in current_dst:
                current_dst[key] = current_src[key]
            else:
                if quacks_like_dict(current_src[key]) and quacks_like_dict(current_dst[key]):
                    stack.append((current_dst[key], current_src[key]))
                else:
                    current_dst[key] = current_src[key]
    return dst


##############################################################################
# DEPLOYMENT PARSING
##############################################################################

class App(object):
    def __init__(self, name, processapp, config, bootlevel, ha_config=None):
        self.name = name
        self.processapp = processapp
        self.config = config

        self.bootlevel = bootlevel
        self.ha_config = ha_config


class Deployment(object):
    def __init__(self, bootlevels, execution_engines,
        default_execution_engine_id, pyon_config):
        self.bootlevels = bootlevels
        self.execution_engines = execution_engines
        self.default_execution_engine_id = default_execution_engine_id
        self.pyon_config = pyon_config


def parse_deployment(rel, launch, ignore_bootlevels=False, no_ha=False):
    """Load app and launch information to produce a deployment map
    """

    try:
        rel_apps = rel['apps']
    except KeyError, e:
        raise PlanError("REL file missing '%s' block" % str(e))

    launch_apps = launch.get('apps')
    launch_app_defaults = launch.get('app_defaults')

    # sort apps into bootlevels and assemble process definitions

    apps = {}
    bootlevel_apps = defaultdict(list)
    for ndex, app_dict in enumerate(rel_apps):
        app_name = app_dict.get('name')
        if not app_name:
            raise PlanError("REL app %d has no name" % ndex)

        if app_name in apps:
            raise PlanError("REL file contains duplicate app '%s'" % app_name)

        processapp = app_dict.get('processapp')
        if processapp is None:
            raise PlanError("app '%s' in REL missing processapp" % (app_name))
        if len(processapp) != 3:
            raise PlanError("app '%s' processapp is invalid" % (app_name))

        config = app_dict.get('config', {})

        if ignore_bootlevels:
            bootlevel = ndex + 1
        else:
            try:
                bootlevel = int(app_dict['bootlevel'])
            except KeyError:
                raise PlanError("app '%s' missing bootlevel in REL" % app_name)
            except ValueError, e:
                raise PlanError("app '%s' bootlevel value in REL is invalid: %s" %
                    (app_name, e))

        # now grab deploy information from the launch file
        launch_app = launch_apps.get(app_name)
        if launch_app is None:
            if launch_app_defaults:
                launch_app = deepcopy(launch_app_defaults)
            else:
                raise PlanError("app '%s' not in launch and there are no app_defaults"
                    % app_name)

        if not launch_app.get('include', True):
            # skip this app if launch file has include=False
            continue

        if app_name == "process_dispatcher":
            # always skip the process dispatcher. it is started out of band
            continue

        if no_ha:
            ha_config = None
        else:
            ha_config = launch_app.get('ha')

        app = App(app_name, processapp, config, bootlevel, ha_config)
        apps[app_name] = app
        bootlevel_apps[bootlevel].append(app)

    # pick out execution engine definitions from launch file
    try:
        engines = launch['execution_engines']
        default_engine_id = launch['default_execution_engine']
    except KeyError, e:
        raise PlanError("Launch file missing '%s' block" % str(e))

    if not engines:
        raise PlanError("Launch file must have at least one execution engine")

    if not default_engine_id in engines:
        raise PlanError("default execution engine '%s' definition missing" %
            default_engine_id)

    bootlevels = [bootlevel_apps[lvl] for lvl in sorted(bootlevel_apps.keys())]

    # pick out pyon config block from launch file
    pyon_config = launch.get('config')
    if pyon_config:
        if not isinstance(pyon_config, collections.Mapping):
            raise PlanError("Launch file has invalid config value. Must be a dictionary.")
    else:
        pyon_config = None

    return Deployment(bootlevels, engines, default_engine_id, pyon_config)


##############################################################################
# PLAN GENERATION
##############################################################################

def generate_plan(pathfinder, profile, deployment, output_dir, force=False,
    use_links=False, no_cleanup=False):
    """Creates a launch plan from a deployment description, in a directory
    """
    output_dir = os.path.normpath(output_dir)
    output_parent_dir = os.path.dirname(output_dir)

    # create temp directory to build plan in. Create in the same directory
    # as the output path so we can rename it into place
    tmpdir = tempfile.mkdtemp(prefix="tmpPlan", dir=output_parent_dir)

    try:
        _inner_generate_plan(pathfinder, profile, deployment, tmpdir,
            use_links=use_links)

        # move the tmpdir into place
        try:
            os.rename(tmpdir, output_dir)
        except OSError, e:
            if e.errno == errno.ENOTEMPTY and force:
                # if output dir exists and force is enabled, remove and retry

                shutil.rmtree(output_dir)
                os.rename(tmpdir, output_dir)
            else:
                raise

    finally:
        # make sure tmpdir is cleaned up
        if not no_cleanup and os.path.exists(tmpdir):
            try:
                shutil.rmtree(tmpdir)
            except Exception:
                pass


def _inner_generate_plan(pathfinder, profile, deployment, output_dir,
    use_links):

    _write_base_files(pathfinder, profile, output_dir, use_links)

    # write and tar process definitions
    pd_json_template = pathfinder.get_template("process_definition.json")
    process_definition_dir = os.path.join(output_dir, "pd-bootstrap",
            "pd-bootstrap", "process-definitions")

    # ensure process definition output directory exists
    os.mkdir(process_definition_dir)

    # ha agent is special case added as a process definition
    _make_process_definition(
            HAAGENT_DEFINITION_NAME,
            "ion.agents.cei.high_availability_agent", "HighAvailabilityAgent",
            pd_json_template, process_definition_dir)

    for level in deployment.bootlevels:
        for app in level:
            _, module, cls = app.processapp
            _make_process_definition(app.name,
                module, cls, pd_json_template, process_definition_dir)

    _tar_process_definitions(output_dir)

    # write boot levels
    level_config_paths = _write_boot_levels(pathfinder, deployment.bootlevels,
        output_dir, HAAGENT_DEFINITION_NAME)

    # write profile-specific config and top level plan
    profile.write_plan(deployment, output_dir, level_config_paths)

    # deep merge pyon configs and write to plan
    pyon_config = pathfinder.get_pyon_config()
    if deployment.pyon_config:
        pyon_config = dict_merge(pyon_config, deployment.pyon_config)
    pyon_config_path = os.path.join(output_dir, "common", "pyon.yml")
    with open(pyon_config_path, 'w') as f:
        yaml.dump(pyon_config, f, default_flow_style=False)


def _write_boot_levels(pathfinder, bootlevels, output_dir, haagent_definition_name):

    json_template = pathfinder.get_template("process.json")
    conf_template = pathfinder.get_template("pyon.conf")
    haagent_template = pathfinder.get_template("haagent_process.json")

    level_index = 0
    level_configs = []

    for level in bootlevels:
        if len(level) == 1:
            name = level[0].name
            level_name = "%s%02d_%s" % (PYONAPP_PREFIX, level_index, name)
        else:
            level_name = "%s%02d" % (PYONAPP_PREFIX, level_index)

        level_directory = level_name
        conf_filename = "%s.conf" % level_name
        conf_relative_path = os.path.join(level_directory, conf_filename)
        level_directory_path = os.path.join(output_dir, level_directory)
        os.mkdir(level_directory_path)

        conf_contents = ""
        for app in level:
            name = app.name

            haagent_dashi_name = "ha_%s" % name
            ha_config = app.ha_config

            # write cloudinitd bootconf and [service] block for app --
            # with or without HA
            if ha_config:
                app_conf = conf_template.substitute(name=name,
                    definition_name=haagent_definition_name,
                    haagent_dashi_name=haagent_dashi_name,
                    restart_mode="ALWAYS")

                policy_name = ha_config['policy']
                policy_params = ha_config['parameters']

                policy_params_json = json.dumps(policy_params, indent=8)
                process_config_json = json.dumps(app.config, indent=4)
                json_contents = haagent_template.substitute(
                    policy_name=policy_name,
                    policy_parameters=policy_params_json,
                    haagent_dashi_name=haagent_dashi_name,
                    process_definition_name=app.name,
                    process_config=process_config_json,
                    resource_id=uuid.uuid4().hex)

            else:
                app_conf = conf_template.substitute(
                    name=name,
                    definition_name=app.name,
                    haagent_dashi_name="",
                    restart_mode="ABNORMAL")
                process_config_json = json.dumps(app.config, indent=2)
                json_contents = json_template.substitute(
                    process_config=process_config_json)

            conf_contents += app_conf + "\n"

            json_filename = "%s_%s.json" % (PYONAPP_PREFIX, name)
            json_path = os.path.join(level_directory_path, json_filename)
            with open(json_path, "w") as json_file:
                json_file.write(json_contents)

        conf_path = os.path.join(level_directory_path, conf_filename)
        with open(conf_path, "w") as conf_file:
            conf_file.write(conf_contents)

        level_configs.append(conf_relative_path)
        level_index += 1

    return level_configs


def _write_base_files(pathfinder, profile, output_dir, use_links):
    # profile is included but unused so far. Thinking that we may want to
    # selectively write only a portion of the base files which are needed
    # for the chosen profile.

    baseplan_path = pathfinder.get_baseplan_path()
    for f in os.listdir(baseplan_path):
        src = os.path.join(baseplan_path, f)
        dst = os.path.join(output_dir, f)
        if use_links:
            os.symlink(src, dst)
        elif os.path.isdir(src):
            shutil.copytree(src, dst, symlinks=True)
        elif os.path.islink(src):
            os.symlink(os.readlink(src), dst)
        else:
            shutil.copy2(src, dst)


def _make_process_definition(name, module, klass, pd_template, definition_dir):
    """Create or update process definition file.
    """

    process_definition_filename = "%s.yml" % name
    process_definition_file_path = os.path.join(definition_dir,
            process_definition_filename)

    pd = pd_template.substitute(name=name, module=module, klass=klass)

    with open(process_definition_file_path, "w") as pd_file:
        pd_file.write(pd)


def _tar_process_definitions(output_dir):
    make_tarball_exe = os.path.join(
        output_dir, "pd-bootstrap", "prepare-tarball.sh")

    check_call(make_tarball_exe)


##############################################################################
# CLI-specific
##############################################################################

def error(msg, exit_code=1):
    print >> sys.stderr, "\nFATAL ERROR:"
    print >> sys.stderr, msg
    sys.exit(exit_code)


def _inner_main(opts):
    pathfinder = PathFinder()
    plandir = os.path.normpath(opts.plandir)
    plandir_parent = os.path.dirname(plandir)

    # bail out early if output dir exists and force=false
    if os.path.exists(plandir):
        if not opts.force:
            raise PlanError("plan output path exists: %s - use --force to overwrite" % plandir)

        elif not os.path.isdir(plandir):
            raise PlanError("plan output path exists and is not a directory: %s" % plandir)

    elif not os.path.exists(plandir_parent):
        raise PlanError("plan output directory parent does not exist: %s" % plandir_parent)

    # read and validate input files
    try:
        rel = yaml.load(opts.relfile)
        opts.relfile.close()
    except Exception, e:
        raise PlanError("failed to read rel file: %s" % str(e))

    try:
        launch = yaml.load(opts.launchfile)
        opts.launchfile.close()
    except Exception, e:
        raise PlanError("failed to read launch file: %s" % str(e))

    try:
        profile_config = yaml.load(opts.profile)
        opts.profile.close()
    except Exception, e:
        raise PlanError("failed to read profile: %s" % str(e))

    # load base profile config and merge user config into it
    base_profile_config = pathfinder.get_profile_defaults()
    profile_config = dict_merge(base_profile_config, profile_config)

    # parse profile and instantiate the appropriate profile object
    try:
        profile_type = profile_config.pop('profile_type')
    except Exception, e:
        raise PlanError("profile is missing profile_type")

    profile_cls = LAUNCH_PROFILE_TYPES.get(profile_type)
    if not profile_cls:
        raise PlanError("profile has invalid type '%s'. See --help for valid types." % profile_type)
    profile = profile_cls(pathfinder)
    profile.load_config(profile_config)

    # parse the Deployment object from the rel and launch configs
    deployment = parse_deployment(rel, launch, opts.ignore_bootlevels, opts.no_ha)

    # now generate the actual plan
    generate_plan(pathfinder, profile, deployment, plandir, force=opts.force,
        no_cleanup=opts.no_cleanup)


def _get_profile_type_descriptions():
    lines = []

    for profile_name in sorted(LAUNCH_PROFILE_TYPES.keys()):
        profile_cls = LAUNCH_PROFILE_TYPES[profile_name]
        helpstr = profile_cls.__doc__
        if helpstr:
            helpstr = helpstr.strip()
        if helpstr:
            lines.append("  %s - %s" % (profile_name, helpstr))
        else:
            lines.append("  %s" % profile_name)

    return "\n".join(lines)


_USAGE = """
%(prog)s [opts] --profile profile.yml --rel rel.yml --launch launch.yml plandir
"""

_USAGE_DESCRIPTION = """

Generates OOICI launch plans

The profile document contains a profile_type value. Supported types are:
""" + _get_profile_type_descriptions() + """

plandir is the path to which the generated plan is written. If the path
exists and the --force flag is used, the directory will be deleted and
replaced.

"""


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=_USAGE_DESCRIPTION, usage=_USAGE)

    required_group = parser.add_argument_group('required arguments')
    required_group.add_argument('--profile', '-p', metavar="profile.yml",
        dest="profile", type=argparse.FileType('r'), required=True)
    required_group.add_argument('--rel', '-r', metavar='rel.yml',
        dest="relfile", type=argparse.FileType('r'), required=True)
    required_group.add_argument('--launch', '-l', metavar='launch.yml',
        dest="launchfile", type=argparse.FileType('r'), required=True)
    required_group.add_argument('plandir', metavar='output path',
        help="plan output directory")

    parser.add_argument('-f', '--force', action='store_const',
        const=True, help="overwrite plan directory if it exists")
    parser.add_argument('-i', '--ignore-bootlevels',
        dest='ignore_bootlevels', action='store_const', const=True,
        help="ignore bootlevels in rel and generate one level per app")
    parser.add_argument('--no-ha', dest='no_ha', action='store_const',
        const=True, help="disable HA agents and launch one process per app")
    # TODO this is disabled. some dirs get written too after being copied,
    # so we might need to "deep" link these dirs instead.
    # parser.add_argument('-u', "--use-links", action='store_const',
    #     const=True,
    #     help="link base files instead of copying. For plan development.")
    parser.add_argument('--no-cleanup', dest='no_cleanup',
        action='store_const', const=True, default=False,
        help="Don't clean up temp files on Error. For development.")

    opts = parser.parse_args()

    try:

        _inner_main(opts)

    except PlanError, e:
        error(str(e))

    except KeyboardInterrupt:
        pass


if __name__ == '__main__':
    main()
